{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Field of Study Top Authors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as sf \n",
    "\n",
    "\n",
    "rootpath = 'wasbs://mag-2018-09-27@magtrainingsource.blob.core.windows.net/mag/'\n",
    "outputDir = '/output/jiaxin/pyspark/'\n",
    "targetFoS = 'computer science'\n",
    "n_top = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark context\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "# load data\n",
    "FieldsOfStudy = spark.read.load(rootpath + \"FieldsOfStudy.txt\", format=\"csv\", sep=\"\\t\") \\\n",
    ".toDF(\"FieldOfStudyId\", \"Rank\", \"NormalizedName\", \"DisplayName\", \"MainType\", \n",
    "      \"Level\", \"PaperCount\", \"CitationCount\", \"CreatedDate\") \\\n",
    ".where(sf.col(\"NormalizedName\").isin(targetFoS)) \\\n",
    ".select(\"FieldOfStudyId\")\n",
    "\n",
    "\n",
    "# Get all paperIds for the field\n",
    "PaperFieldsOfStudy = spark.read.load(rootpath + \"PaperFieldsOfStudy.txt\", format=\"csv\", sep=\"\\t\") \\\n",
    ".toDF(\"PaperId\", \"FieldOfStudyId\", \"Score\") \\\n",
    ".join(FieldsOfStudy, \"FieldOfStudyId\", 'inner') \\\n",
    ".select(\"PaperId\")\n",
    "\n",
    "\n",
    "# Get all [citing paper] -> [field paper] relationships\n",
    "PaperReferences = spark.read.load(rootpath + \"PaperReferences.txt\", format=\"csv\", sep=\"\\t\") \\\n",
    ".toDF(\"PaperId\", \"PaperReferenceId\") \\\n",
    ".groupby(\"PaperReferenceId\") \\\n",
    ".count() \\\n",
    ".selectExpr(\"PaperReferenceId as PaperId\", \"count as CitationCount\")\n",
    "\n",
    "\n",
    "# For each field paper, count incoming reference to get citation count\n",
    "CitationCount = PaperReferences.join(PaperFieldsOfStudy, \"PaperId\", 'inner')\n",
    "\n",
    "\n",
    "# Join against PaperAuthorAffiliation to get field paper -> field author relationship\n",
    "PaperAuthorAffiliation = spark.read.load(rootpath + \"PaperAuthorAffiliations.txt\", format=\"csv\", sep=\"\\t\") \\\n",
    ".toDF(\"PaperId\", \"AuthorId\", \"AffiliationId\", \"AuthorSequenceNumber\", \"OriginalAffiliation\") \\\n",
    ".join(CitationCount, \"PaperId\", 'inner') \\\n",
    ".groupby(\"AuthorId\") \\\n",
    ".agg(sf.sum(\"CitationCount\").alias(\"CitationCount\")) \\\n",
    ".select(\"AuthorId\", \"PaperId\", \"CitationCount\") \\\n",
    ".show()\n",
    "\n",
    "\n",
    "# Then join against Author to get detail author information\n",
    "Authors = spark.read.load(rootpath + \"Authors.txt\", format=\"csv\", sep=\"\\t\") \\\n",
    ".toDF(\"AuthorId\", \"Rank\", \"NormalizedName\", \"DisplayName\", \"LastKnownAffiliationId\", \n",
    "      \"PaperCount\", \"CitationCount\", \"CreatedDate\") \\\n",
    ".select(\"AuthorId\", \"DisplayName\", \"PaperId\") \\\n",
    ".join(PaperAuthorAffiliation, \"AuthorId\", 'inner') \\\n",
    ".selectExpr(\"AuthorId as AuthorId\", \"DisplayName as Name\", \"PaperId as PaperId\", \"CitationCount as CitationCount\")\n",
    "\n",
    "\n",
    "# Aggregate over authorId to cacluate publication/ciation count for each author\n",
    "PublicationCount = Authors.groupby(\"AuthorId\") \\\n",
    ".count() \\\n",
    ".selectExpr(\"AuthorId as AuthorId\", \"count as PublicationCount\") \\\n",
    ".join(Authors, \"AuthorId\", 'inner') \\\n",
    ".select(\"AuthorId\", \"Name\", \"CitationCount\", \"PublicationCount\") \\\n",
    ".dropDuplicates().show()\n",
    "\n",
    "\n",
    "# Get top n authors by citation count\n",
    "Top_Authors = PublicationCount.sort(sf.desc(\"CitationCount\"), sf.desc(\"PublicationCount\")) \\\n",
    ".limit(n_top)\n",
    "\n",
    "# Save results\n",
    "Top_Authors.write.csv(outputDir + \"Top_Authors.csv\", header=True)\n",
    "\n",
    "\n",
    "# Stop Spark context\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
